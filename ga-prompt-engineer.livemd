# prompt-engineer-ga

## Introduction

<!-- Learn more at https://mermaid-js.github.io/mermaid -->

```mermaid
flowchart TD
    Start[Start Program] --> Init[Initialize Population of Prompts]
    Init --> EvLoop{Evolution Loop}
    
    subgraph Generation[Generation Cycle]
        EvLoop --> Eval[Evaluate Population]
        
        subgraph Evaluation[Evaluation Process]
            Eval --> SendP[Send Prompt to Ollama]
            SendP --> GenCode[Generate Code]
            GenCode --> EvalCode[Evaluate Generated Code]
            EvalCode --> AssignFit[Assign Fitness Score]
        end
        
        AssignFit --> Select[Select Parents]
        Select --> Cross[Crossover]
        Cross --> Mutate[Mutation]
        Mutate --> NewPop[Create New Population]
    end
    
    NewPop --> CheckGen{Check Generation Count}
    CheckGen -->|Max Generations Not Reached| EvLoop
    CheckGen -->|Max Generations Reached| End[Return Best Prompt]
    
    subgraph Components[Prompt Components]
        PC[Template Parts]
        PC --> Context[Context Setting]
        PC --> Task[Task Description]
        PC --> Reqs[Requirements]
        PC --> Output[Output Specification]
        PC --> Lang[Language Choice]
        PC --> Extra[Additional Instructions]
    end
```

<!-- livebook:{"break_markdown":true} -->

This program implements a genetic algorithm to evolve and optimize prompts for generating code through Ollama. How it works and its key components:

The program operates like a selective breeding process for prompts, similar to how nature evolves organisms over generations. Instead of breeding plants or animals, we're evolving prompts to generate better code. Here's how it works:

First, the program creates prompts by combining different components. Think of these components as building blocks for our prompts. Each prompt is made up of six essential parts:

1. Context Setting: This introduces the request (e.g., "As an expert programmer")
2. Task Description: This explains what needs to be done (e.g., "implement a function to")
3. Requirements: This specifies quality expectations (e.g., "is efficient and well-documented")
4. Output Specification: This indicates how to present the solution
5. Language Choice: This specifies the programming language to use
6. Additional Instructions: This provides extra guidance or requirements

The evolution process works through several key steps:

1. Population Initialization: The program starts by creating multiple different prompts by randomly combining these components. This is like creating an initial population of organisms with different genetic traits.

2. Evaluation Process: Each prompt goes through a two-step evaluation:

   * First, the prompt is sent to Ollama, which generates code based on the prompt
   * Then, the generated code is evaluated using a separate evaluation prompt that scores various aspects like clarity, efficiency, correctness, style, and documentation

3. Selection and Evolution: The program then uses several genetic algorithm techniques:

   * Elite Selection: The best-performing prompts are automatically kept for the next generation
   * Tournament Selection: Prompts compete in small groups to become parents
   * Crossover: Parts of two parent prompts are combined to create new prompts
   * Mutation: Random changes are introduced to maintain diversity

4. Iteration: This process repeats for multiple generations, with each generation potentially producing better prompts than the last.

The program uses several parameters to control this evolution:

* Population size determines how many prompts exist in each generation
* Maximum generations sets how long the evolution runs
* Mutation rate controls how often random changes occur
* Elite size determines how many top performers automatically survive
* Tournament size affects how selective the breeding process is

The goal is to find prompts that consistently generate high-quality code. The fitness score comes from evaluating the actual code that Ollama generates, not just the prompt itself. This ensures we're optimizing for real-world results rather than just how the prompt looks.

The accompanying diagram shows this flow, illustrating how prompts move through the system, get evaluated, and evolve over time. The left side shows the main evolution loop, while the right side shows the components that make up each prompt. The evaluation process is highlighted in its own section to show the important two-step process of generating and then evaluating code.

This approach is particularly powerful because it can discover prompt patterns that humans might not think of. By trying many combinations and keeping what works best, the system can find increasingly effective ways to instruct the AI model to generate high-quality code.

Would you like me to elaborate on any particular aspect of how the program works?

```elixir
# First, let's set up our dependencies
Mix.install([
  {:httpoison, "~> 2.0"},
  {:jason, "~> 1.4"}
])

defmodule Individual do
  @moduledoc """
  Represents a single prompt individual in our population.
  Each individual contains a prompt template made up of different components
  that can be mutated and evolved.
  """
  defstruct [:template_parts, :fitness]
end

defmodule PromptEvaluator do
  @moduledoc """
  Handles communication with Ollama API and evaluates the quality of generated code
  based on various metrics.
  """
  @base_url "http://localhost:11434/api"
  
  def evaluate_prompt_result(generated_code) do
    # Here we evaluate the quality of the code generated by our prompt
    evaluation_prompt = """
    Evaluate this code and provide scores in the following format:
    
    SCORES (1-10):
    Clarity: [score]
    Efficiency: [score]
    Correctness: [score]
    Style: [score]
    Documentation: [score]
    
    Provide a brief explanation after each score.
    Calculate and show the TOTAL SCORE as an average.
    
    CODE TO EVALUATE:
    #{generated_code}
    """
    
    case generate_with_ollama(evaluation_prompt) do
      {:ok, evaluation_text} -> extract_score(evaluation_text)
      {:error, _} -> {0.0, "Failed to evaluate"}
    end
  end

  def generate_with_ollama(prompt) do
    url = @base_url <> "/generate"
    
    body = %{
      model: "codellama:7b",
      prompt: prompt,
      stream: false,
      options: %{
        num_predict: 1000
      }
    }
    
    try do
      response = HTTPoison.post!(url, Jason.encode!(body), 
        [{"Content-Type", "application/json"}],
        [timeout: 60_000, recv_timeout: 60_000])
        
      with {:ok, decoded} <- Jason.decode(response.body),
           %{"response" => text} <- decoded do
        {:ok, text}
      else
        _ -> {:error, "Failed to decode response"}
      end
    rescue
      e -> {:error, "HTTP error: #{inspect(e)}"}
    end
  end
  
  defp extract_score(evaluation_text) do
    case Regex.run(~r/(?:TOTAL SCORE|Average)[:\s]+(\d+\.?\d*)/, evaluation_text) do
      [_, score] ->
        case Float.parse(score) do
          {num, _} -> {num, evaluation_text}
          :error -> {0.0, "Failed to parse score"}
        end
      nil -> {0.0, "No score found"}
    end
  end
end

defmodule PromptGA do
  @moduledoc """
  Implements the genetic algorithm for evolving prompts.
  """
  
  # Define template parts for constructing prompts
  @template_parts [
    # Context setting
    ["Given a programming task,", "As an expert programmer,", "You are a skilled developer,"],
    
    # Task description
    ["write code that", "implement a function to", "create a program that"],
    
    # Requirements
    ["is efficient and well-documented.", "follows best practices and includes error handling.", "is clean, maintainable, and properly tested."],
    
    # Output specification
    ["Provide the solution in", "Write the implementation using", "Show the code in"],
    
    # Language specification
    ["Python", "JavaScript", "Ruby"],
    
    # Additional instructions
    ["Include helpful comments.", "Add documentation for key components.", "Explain any complex logic."]
  ]

  def run(config) do
    population = initialize_population(config.population_size)
    run_evolution(population, config, 1)
  end

  defp initialize_population(size) do
    for _ <- 1..size do
      %Individual{
        template_parts: Enum.map(@template_parts, &Enum.random/1),
        fitness: nil
      }
    end
  end

  defp run_evolution(population, config, generation) do
    evaluated_pop = evaluate_population(population)
    best = Enum.max_by(evaluated_pop, & &1.fitness)
    
    IO.puts("\nGeneration #{generation}")
    IO.puts("Best Fitness: #{best.fitness}")
    IO.puts("Best Prompt:")
    IO.puts(create_prompt(best))

    if generation >= config.max_generations do
      best
    else
      new_population = 
        evolve_population(evaluated_pop, config)
        |> Enum.map(&maybe_mutate(&1, config.mutation_rate))

      run_evolution(new_population, config, generation + 1)
    end
  end

  defp evaluate_population(population) do
    Enum.map(population, &evaluate_individual/1)
  end

  defp evaluate_individual(individual) do
    prompt = create_prompt(individual)
    
    # First, generate code using the prompt
    case PromptEvaluator.generate_with_ollama(prompt) do
      {:ok, generated_code} ->
        # Then evaluate the generated code
        {score, _explanation} = PromptEvaluator.evaluate_prompt_result(generated_code)
        %{individual | fitness: score}
      {:error, _} ->
        %{individual | fitness: 0.0}
    end
  end

  def create_prompt(individual) do
    Enum.join(individual.template_parts, " ")
  end

  defp evolve_population(population, config) do
    elites = 
      population
      |> Enum.sort_by(& &1.fitness, :desc)
      |> Enum.take(config.elite_size)

    num_children = config.population_size - config.elite_size

    children =
      for _ <- 1..num_children do
        parent1 = select_parent(population, config.tournament_size)
        parent2 = select_parent(population, config.tournament_size)
        crossover(parent1, parent2)
      end

    elites ++ children
  end

  defp select_parent(population, tournament_size) do
    population
    |> Enum.take_random(tournament_size)
    |> Enum.max_by(& &1.fitness)
  end

  defp crossover(parent1, parent2) do
    point = :rand.uniform(length(parent1.template_parts))
    {parts1, parts2} = Enum.split(parent1.template_parts, point)
    {parts3, parts4} = Enum.split(parent2.template_parts, point)
    
    %Individual{
      template_parts: parts1 ++ parts4,
      fitness: nil
    }
  end

  defp maybe_mutate(individual, rate) do
    template_parts = Enum.map(Enum.with_index(individual.template_parts), fn {part, idx} ->
      if :rand.uniform() < rate do
        Enum.random(Enum.at(@template_parts, idx))
      else
        part
      end
    end)
    
    %{individual | template_parts: template_parts, fitness: nil}
  end
end

# Configuration for running the algorithm
config = %{
  population_size: 10,    # Start with a small population for testing
  max_generations: 5,     # Fewer generations for initial testing
  mutation_rate: 0.1,     # 10% chance of mutation
  elite_size: 2,         # Keep the top 2 individuals
  tournament_size: 3      # Tournament selection size
}

IO.puts("Starting Prompt Evolution...")
best_solution = PromptGA.run(config)

IO.puts("\nFinal Best Solution:")
IO.puts("==================")
IO.puts("Fitness: #{best_solution.fitness}")
IO.puts("\nPrompt:")
IO.puts(PromptGA.create_prompt(best_solution))
```
